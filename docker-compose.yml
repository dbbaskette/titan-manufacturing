# =============================================================================
# TITAN MANUFACTURING â€” Titan 5.0 AI Platform
# "Forging the future with intelligent manufacturing"
# =============================================================================
# Simplified architecture: Greenplum as single data store
# RabbitMQ provides both AMQP and MQTT (via plugin)
# =============================================================================

# =============================================================================
# YAML ANCHORS - DRY Configuration
# =============================================================================
x-greenplum-env: &greenplum-env
  GREENPLUM_URL: jdbc:postgresql://greenplum:5432/titan-manufacturing
  GREENPLUM_USER: ${GREENPLUM_USER:-gpadmin}
  GREENPLUM_PASSWORD: ${GREENPLUM_PASSWORD:-VMware1!}

x-mqtt-env: &mqtt-env
  MQTT_BROKER: tcp://rabbitmq:1883
  MQTT_USER: ${RABBITMQ_USER:-titan}
  MQTT_PASS: ${RABBITMQ_PASSWORD:-titan5.0}

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

x-healthcheck-spring: &healthcheck-spring
  interval: 10s
  timeout: 5s
  retries: 5
  start_period: 30s

services:
  # ===========================================================================
  # CORE INFRASTRUCTURE (Increment 1)
  # ===========================================================================

  # Greenplum - Single unified database for all data
  # Using full image with all extensions (pgvector, PostGIS, MADlib, etc.)
  greenplum:
    image: greenplum-sne-full
    container_name: titan-greenplum
    hostname: greenplum-sne
    ports:
      - "15432:5432"
    environment:
      - PGPASSWORD=${GREENPLUM_PASSWORD:-VMware1!}
    volumes:
      - greenplum-data:/data
      - ./config/greenplum/init.sql:/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "su - gpadmin -c '/usr/local/greenplum-db/bin/psql -U gpadmin -d postgres -c \"SELECT 1\"' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - titan-network

  # Greenplum database initialization
  # Creates titan-manufacturing database and runs init.sql
  # MCP servers depend on this completing successfully
  greenplum-init:
    image: greenplum-sne-full
    container_name: titan-greenplum-init
    depends_on:
      greenplum:
        condition: service_healthy
    volumes:
      - ./config/greenplum/init.sql:/init.sql:ro
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "Checking if database exists..."
        if ! /usr/local/greenplum-db/bin/psql -h greenplum -U gpadmin -d postgres -tc "SELECT 1 FROM pg_database WHERE datname = 'titan-manufacturing'" | grep -q 1; then
          echo "Creating titan-manufacturing database..."
          /usr/local/greenplum-db/bin/psql -h greenplum -U gpadmin -d postgres -c "CREATE DATABASE \"titan-manufacturing\";"
          echo "Running init.sql..."
          /usr/local/greenplum-db/bin/psql -h greenplum -U gpadmin -d titan-manufacturing -f /init.sql 2>&1 || true
          echo "Database initialization complete!"
        else
          echo "Database titan-manufacturing already exists, skipping init."
        fi
    logging: *default-logging
    networks:
      - titan-network

  # RabbitMQ - Message broker with MQTT plugin enabled
  # Provides both AMQP (5672) and MQTT (1883) protocols
  rabbitmq:
    image: rabbitmq:3-management
    container_name: titan-rabbitmq
    ports:
      - "5672:5672"     # AMQP
      - "15672:15672"   # Management UI
      - "1883:1883"     # MQTT (via plugin)
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER:-titan}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD:-titan5.0}
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
      - ./config/rabbitmq/enabled_plugins:/etc/rabbitmq/enabled_plugins
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_running"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    networks:
      - titan-network

  # ===========================================================================
  # TITAN 5.0 ORCHESTRATOR (Increment 2+)
  # ===========================================================================
  titan-orchestrator:
    build: ./titan-orchestrator
    container_name: titan-orchestrator
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      # Use Ollama via OpenAI-compatible endpoint
      - OPENAI_API_KEY=${OPENAI_API_KEY:-ollama}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-http://host.docker.internal:11434}
      - OPENAI_MODEL=${OPENAI_MODEL:-llama3.1:8b}
      - OPENAI_TEMPERATURE=${OPENAI_TEMPERATURE:-0.7}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - DEFAULT_LLM=${DEFAULT_LLM:-gpt-4.1}
      # MCP Server connections
      - MCP_SERVERS_SENSOR=http://sensor-mcp-server:8081
      - MCP_SERVERS_MAINTENANCE=http://maintenance-mcp-server:8082
      - MCP_SERVERS_INVENTORY=http://inventory-mcp-server:8083
      - MCP_SERVERS_LOGISTICS=http://logistics-mcp-server:8084
      - MCP_SERVERS_ORDER=http://order-mcp-server:8085
      - MCP_SERVERS_COMMUNICATIONS=http://communications-mcp-server:8086
      - MCP_SERVERS_GOVERNANCE=http://governance-mcp-server:8087
      - MAINTENANCE_URL=http://maintenance-mcp-server:8082
    depends_on:
      greenplum:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      # Wait for MCP servers to be healthy before starting
      sensor-mcp-server:
        condition: service_healthy
      maintenance-mcp-server:
        condition: service_healthy
      inventory-mcp-server:
        condition: service_healthy
      logistics-mcp-server:
        condition: service_healthy
      order-mcp-server:
        condition: service_healthy
      communications-mcp-server:
        condition: service_healthy
      governance-mcp-server:
        condition: service_healthy
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    networks:
      - titan-network
    profiles:
      - orchestrator

  # ===========================================================================
  # MCP AGENT SERVERS (Increment 2+)
  # ===========================================================================
  sensor-mcp-server:
    build: ./sensor-mcp-server
    container_name: titan-sensor-agent
    hostname: sensor-mcp-server
    ports:
      - "8081:8081"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      <<: [*greenplum-env, *mqtt-env]
      MQTT_ENABLED: "true"
    depends_on:
      greenplum-init:
        condition: service_completed_successfully
      rabbitmq:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8081/actuator/health || exit 1"]
      <<: *healthcheck-spring
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - agents

  maintenance-mcp-server:
    build: ./maintenance-mcp-server
    container_name: titan-maintenance-agent
    hostname: maintenance-mcp-server
    ports:
      - "8082:8082"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      <<: [*greenplum-env, *mqtt-env]
      GEMFIRE_LOCATOR_HOST: gemfire
      GEMFIRE_LOCATOR_PORT: "10334"
    depends_on:
      greenplum-init:
        condition: service_completed_successfully
      rabbitmq:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8082/actuator/health || exit 1"]
      <<: *healthcheck-spring
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - agents

  sensor-data-generator:
    build: ./sensor-data-generator
    container_name: titan-sensor-generator
    ports:
      - "8090:8090"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      <<: *mqtt-env
      EQUIPMENT_COUNT: "72"
      FACILITIES: PHX,MUC,SHA,DET,ATL,DAL,LYN,MAN,MEX,SEO,SYD,TYO
      INTERVAL_MS: "5000"
    depends_on:
      rabbitmq:
        condition: service_healthy
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    networks:
      - titan-network
    profiles:
      - generator

  inventory-mcp-server:
    build: ./inventory-mcp-server
    container_name: titan-inventory-agent
    hostname: inventory-mcp-server
    ports:
      - "8083:8083"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      <<: *greenplum-env
    depends_on:
      greenplum-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8083/actuator/health || exit 1"]
      <<: *healthcheck-spring
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - agents

  logistics-mcp-server:
    build: ./logistics-mcp-server
    container_name: titan-logistics-agent
    hostname: logistics-mcp-server
    ports:
      - "8084:8084"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      <<: *greenplum-env
    depends_on:
      greenplum-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8084/actuator/health || exit 1"]
      <<: *healthcheck-spring
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - agents

  # ===========================================================================
  # INCREMENT 5 & 6: ORDER FULFILLMENT & DATA GOVERNANCE
  # ===========================================================================
  order-mcp-server:
    build: ./order-mcp-server
    container_name: titan-order-agent
    hostname: order-mcp-server
    ports:
      - "8085:8085"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      <<: *greenplum-env
    depends_on:
      greenplum-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8085/actuator/health || exit 1"]
      <<: *healthcheck-spring
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - agents

  communications-mcp-server:
    build: ./communications-mcp-server
    container_name: titan-comms-agent
    hostname: communications-mcp-server
    ports:
      - "8086:8086"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      <<: *greenplum-env
    depends_on:
      greenplum-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8086/actuator/health || exit 1"]
      <<: *healthcheck-spring
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - agents

  governance-mcp-server:
    build: ./governance-mcp-server
    container_name: titan-governance-agent
    hostname: governance-mcp-server
    ports:
      - "8087:8087"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      <<: *greenplum-env
      OPENMETADATA_URL: ${OPENMETADATA_URL:-http://localhost:8585}
      OPENMETADATA_TOKEN: ${OPENMETADATA_TOKEN:-}
    depends_on:
      greenplum-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8087/actuator/health || exit 1"]
      <<: *healthcheck-spring
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - agents

  # ===========================================================================
  # OPENMETADATA STACK (Increment 6 - Data Governance)
  # Requires ~6GB RAM. Enable with: docker-compose --profile governance up
  # ===========================================================================
  openmetadata:
    image: openmetadata/server:1.3.1
    container_name: titan-openmetadata
    ports:
      - "8585:8585"
    environment:
      - OPENMETADATA_CLUSTER_NAME=titan-manufacturing
      - DB_HOST=openmetadata-mysql
      - DB_PORT=3306
      - DB_USER=${MYSQL_USER:-openmetadata}
      - DB_USER_PASSWORD=${MYSQL_PASSWORD:-openmetadata}
      - DB_DRIVER_CLASS=com.mysql.cj.jdbc.Driver
      - SEARCH_HOST=openmetadata-elasticsearch
      - SEARCH_PORT=9200
      - SEARCH_SCHEME=http
    depends_on:
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8585/api/v1/system/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - titan-network
    profiles:
      - governance

  openmetadata-mysql:
    image: mysql:8.0
    container_name: titan-openmetadata-mysql
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD:-root}
      - MYSQL_DATABASE=openmetadata
      - MYSQL_USER=${MYSQL_USER:-openmetadata}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD:-openmetadata}
    volumes:
      - openmetadata-mysql-data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    networks:
      - titan-network
    profiles:
      - governance

  openmetadata-elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.3
    container_name: titan-openmetadata-es
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - openmetadata-es-data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"\\|\"status\":\"yellow\"'"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    networks:
      - titan-network
    profiles:
      - governance

  # ===========================================================================
  # GEMFIRE (Real-time ML scoring with PMML)
  # Enable with: docker-compose --profile gemfire up
  # Pulse UI: http://localhost:7070
  # ===========================================================================
  gemfire:
    build:
      context: ../gemfire-sne
    image: gemfire-sne
    container_name: titan-gemfire
    ports:
      - "10334:10334"   # Locator
      - "40404:40404"   # Server
      - "7070:7070"     # Pulse web UI
    environment:
      - JAVA_OPTS=-Xmx2g
    volumes:
      - gemfire-data:/data
      - ./config/gemfire/entrypoint.sh:/entrypoint.sh:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:7070/pulse/login.html || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 2G
    networks:
      - titan-network
    profiles:
      - gemfire

  # ===========================================================================
  # OBSERVABILITY (Optional)
  # Enable with: docker-compose --profile observability up
  # ===========================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: titan-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-titan5.0}
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - observability

  prometheus:
    image: prom/prometheus:latest
    container_name: titan-prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    networks:
      - titan-network
    profiles:
      - observability

  # ===========================================================================
  # TITAN DASHBOARD (Increment 7)
  # ===========================================================================
  titan-dashboard:
    build: ./titan-dashboard
    container_name: titan-dashboard
    ports:
      - "3001:3000"
    environment:
      - REACT_APP_ORCHESTRATOR_URL=http://localhost:8080
      - REACT_APP_OPENMETADATA_URL=http://localhost:8585
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3000/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    networks:
      - titan-network
    profiles:
      - dashboard

networks:
  titan-network:
    driver: bridge

volumes:
  greenplum-data:
  rabbitmq-data:
  grafana-data:
  prometheus-data:
  openmetadata-mysql-data:
  openmetadata-es-data:
  gemfire-data:
